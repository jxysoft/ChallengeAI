{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安装部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://www.modelscope.cn/datasets/BJQW14B/bs_challenge_financial_14b_dataset.git\n",
    "!git clone https://github.com/jxysoft/ChallengeAI.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "get_ipython().system = os.system\n",
    "# !nohup python Qwen/openai_api.py -c jxy/Tongyi-Finance-14B-Chat-Int4 &\n",
    "# !nohup python Qwen/openai_api.py -c jxy/Tongyi-Finance-14B-Chat &\n",
    "!nohup python Qwen/openai_api.py -c TongyiFinance/Tongyi-Finance-14B-Chat-Int4 &\n",
    "# !nohup python Qwen/openai_api.py -c Qwen/Qwen-14B-Chat-Int4 --all_gpu &\n",
    "# !nohup python Qwen/openai_api.py -c Qwen/Qwen-7B-Chat &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U torch torchaudio torchdata torchtext torchvision torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken\n",
    "!pip install modelscope\n",
    "!pip install transformers_stream_generator==0.0.4\n",
    "!pip install optimum==1.12.0\n",
    "!pip install auto-gptq\n",
    "# %pip install transformers_stream_generator==0.0.4\n",
    "# %pip install modelscope   #>=1.9.0\n",
    "# %pip install auto-gptq  #==0.4.2\n",
    "# %pip install optimum==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -b v1.0.8 https://github.com/Dao-AILab/flash-attention\n",
    "!cd flash-attention && pip install .\n",
    "# 下方安装可选，安装可能比较缓慢。\n",
    "# Below are optional. Installing them might be slow.\n",
    "# pip install csrc/layer_norm\n",
    "# pip install csrc/rotary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer, snapshot_download\n",
    "from modelscope import GenerationConfig\n",
    "\n",
    "model_dir = snapshot_download('TongyiFinance/Tongyi-Finance-14B-Chat-Int4')\n",
    "\n",
    "# Note: The default behavior now has injection attack prevention off.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer, snapshot_download\n",
    "from modelscope import GenerationConfig\n",
    "\n",
    "model_dir = snapshot_download('TongyiFinance/Tongyi-Finance-14B-Chat-Int4')\n",
    "\n",
    "# Note: The default behavior now has injection attack prevention off.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "# use bf16\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n",
    "# use cpu only\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"cpu\", trust_remote_code=True).eval()\n",
    "# use auto mode, automatically select precision based on the device.\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"cpu\", trust_remote_code=True).eval()\n",
    "# 模型加载指定device_map='cuda:0'，更改成device_map='auto'会使用所有可用显卡\n",
    "\n",
    "# Specify hyperparameters for generation\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "response, history = model.chat(tokenizer, \"请解释一下资产负债率\", history=None)\n",
    "print(response)\n",
    "# 资产负债率是一个财务比率，用来衡量一个企业的负债水平。它是用一个企业负债总额除以其资产总额的百分比来表示的。它的计算公式是：资产负债率 = 负债总额 / 资产总额。它能够反映一个企业的财务状况，以及它是否具有足够的资产来抵偿其债务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer, snapshot_download\n",
    "from modelscope import GenerationConfig\n",
    "\n",
    "model_dir = snapshot_download('TongyiFinance/Tongyi-Finance-14B')\n",
    "\n",
    "# Note: The default behavior now has injection attack prevention off.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "# use bf16\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"cuda:0\", trust_remote_code=True, bf16=True).eval()\n",
    "# use cpu only\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"cpu\", trust_remote_code=True).eval()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"cuda:0\", trust_remote_code=True).eval()\n",
    "# 模型加载指定device_map='cuda:0'，更改成device_map='auto'会使用所有可用显卡\n",
    "\n",
    "# Specify hyperparameters for generation\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "inputs = tokenizer('市盈率是最常用来评估股价水平是否合理的指标之一，是指', return_tensors='pt')\n",
    "inputs = inputs.to(model.device)\n",
    "pred = model.generate(**inputs)\n",
    "print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n",
    "# 市盈率是最常用来评估股价水平是否合理的指标之一，是指股票价格与每股盈利的比率。..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer, snapshot_download\n",
    "from modelscope import GenerationConfig\n",
    "\n",
    "model_dir = snapshot_download('TongyiFinance/Tongyi-Finance-14B-Chat-Int4')\n",
    "# model_dir = snapshot_download('TongyiFinance/Tongyi-Finance-14B-Chat')\n",
    "\n",
    "\n",
    "# Note: The default behavior now has injection attack prevention off.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "print(\"begin load model\")\n",
    "# quantization_config=bnb_config,\n",
    "# use bf16\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True).eval()\n",
    "# none int4\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_dir,  device_map=\"auto\", offload_folder=\"offload\", offload_state_dict = True, trust_remote_code=True, bf16=True).eval()\n",
    "# use cpu only\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"cpu\", trust_remote_code=True).eval()\n",
    "# use auto mode, automatically select precision based on the device.\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"cuda:0\", trust_remote_code=True).eval()\n",
    "# 模型加载指定device_map='cuda:0'，更改成device_map='auto'会使用所有可用显卡\n",
    "print(\"after load model \")\n",
    "# Specify hyperparameters for generation\n",
    "# model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "# response, history = model.chat(tokenizer, \"请解释一下资产负债率\", history=None)\n",
    "# print(response)\n",
    "# 资产负债率是一个财务比率，用来衡量一个企业的负债水平。它是用一个企业负债总额除以其资产总额的百分比来表示的。它的计算公式是：资产负债率 = 负债总额 / 资产总额。它能够反映一个企业的财务状况，以及它是否具有足够的资产来抵偿其债务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel,AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jxy/Tongyi-Finance-14B-Chat-Int4\", \n",
    "                                          device_map=\"auto\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"jxy/Tongyi-Finance-14B-Chat-Int4\", torch_dtype=\"auto\", \n",
    "    offload_folder=\"offload\", device_map=\"cuda:0\", \n",
    "    offload_state_dict = True, max_memory={0:\"14GB\",1:\"14GB\",\"cpu\": \"20GB\"}, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"begin chat\")\n",
    "response, history = model.chat(tokenizer, \"请解释一下资产负债率\", history=None)\n",
    "print(response)\n",
    "response, history = model.chat(tokenizer, \"立讯精密的资产负债率如何\", history=history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(push_to_hub=False, save_directory=\"/root/.cache/modelscope/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(push_to_hub=False, save_directory=\"/root/.cache/modelscope/model\", max_shard_size=\"2GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel,AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-7B-Chat\", device_map=\"auto\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(push_to_hub=False, save_directory=\"/root/.cache/modelscope/model\", max_shard_size=\"2GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
