{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi && nvcc -V\n",
    "# import torch\n",
    "# print(torch.__version__)\n",
    "# torch.cuda.device_count()\n",
    "# import sys\n",
    "# print(sys.version)\n",
    "\n",
    "# kaggle\n",
    "# HOME_PATH='/kaggle/working/ChallengeAI/2023bojin'\n",
    "# model_name = \"Qwen-14B-Chat-Int4\"\n",
    "# model_repo_id=\"Qwen/Qwen-14B-Chat-Int4\"\n",
    "# use_ms=False\n",
    "\n",
    "# modelscope\n",
    "# HOME_PATH = \"/mnt/workspace/ChallengeAI/2023bojin\"\n",
    "# model_name = \"Tongyi-Finance-14B-Chat-Int4\"\n",
    "# model_repo_id=\"TongyiFinance/Tongyi-Finance-14B-Chat-Int4\"\n",
    "# use_ms = True\n",
    "\n",
    "# local mac\n",
    "HOME_PATH=\"/Users/jxy/Projects/person/ChallengeAI/2023bojin\"\n",
    "model_name = \"gpt-3.5-turbo-1106\"\n",
    "model_repo_id= None\n",
    "use_ms=False\n",
    "import sys\n",
    "sys.path.append( HOME_PATH + '/python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安装部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/jxysoft/ChallengeAI.git\n",
    "!git clone https://www.modelscope.cn/datasets/BJQW14B/bs_challenge_financial_14b_dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic \n",
    "# !pip install -U torch==2.0.1 torchdata\n",
    "!pip install -U transformers\n",
    "!pip install tiktoken\n",
    "!pip install transformers_stream_generator  #==0.0.4 \n",
    "!pip install -U sentence-transformers\n",
    "\n",
    "# optimize\n",
    "!git clone -b v1.0.8 https://github.com/Dao-AILab/flash-attention\n",
    "!cd flash-attention && pip install .\n",
    "# !cd flash-attention && pip install flash_attn-1.0.8-cp310-cp310-linux_x86_64.whl\n",
    "# Below are optional. Installing them might be slow.\n",
    "# pip install csrc/layer_norm\n",
    "# pip install csrc/rotary\n",
    "!pip install accelerate\n",
    "\n",
    "!pip install auto-gptq==0.4.2 \n",
    "!pip install optimum==1.12.0\n",
    "!pip install bitsandbytes \n",
    "\n",
    "# !pip install modelscope\n",
    "\n",
    "# open ai\n",
    "!pip install fastapi uvicorn openai \"pydantic>=2.5.0\" sse_starlette\n",
    "!pip install tiktoken einops transformers_stream_generator\n",
    "\n",
    "# app \n",
    "!pip install jsonlines \n",
    "!pip install swifter langchain pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# get_ipython().system = os.system\n",
    "# !nohup python ChallengeAI/2023bojin/openai_api.py -c TongyiFinance/Tongyi-Finance-14B-Chat-Int4 --one_gpu &\n",
    "# !nohup python ChallengeAI/2023bojin/openai_api.py -c Qwen/Qwen-14B-Chat-Int4 &\n",
    "# !nohup python ChallengeAI/2023bojin/openai_api.py -c Qwen/Qwen-7B-Chat &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批量执行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单个执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key='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')\n",
    "# client = OpenAI(base_url='http://127.0.0.1:8000/v1/', api_key='none')\n",
    "from config import project_config\n",
    "project_config.model_name = model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic test\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查询招股书公司名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo\"\n",
    "# 查招股书公司名字\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# normal \n",
    "# file_path = HOME_PATH+\"/data/txt/\" + \"0b46f7a2d67b5b59ad67cafffa0e12a9f0837790.txt\"\n",
    "# abnormal txt\n",
    "\n",
    "# 91b4426b075560a1a45247f9cfa9fa73d56c945c  广州中海达卫星导航技术股份有限公司\n",
    "# 9951262c20ed33562a2fff85c83aeae320f14922  东莞勤上光电股份有限公司 不对，\n",
    "# a244b6ed9da7411f804e62b82a8fdfc015dff284  惠州光弘科技股份有限公司 带后缀\n",
    "# 还有些公司缩写\n",
    "file_path = HOME_PATH+\"/data/txt/\" + \"f587290218d881e18e88fc1431b022b2c5aca81a.txt\"\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    template = ChatPromptTemplate.from_template(\n",
    "                    \"你是一个能精准提取信息的AI。\"\n",
    "                    \"我会给你一篇招股说明书部分内容，请指出它的主体是哪家公司（一般发行人或者发行人简要情况后面的公司名称就是主体）。\"\n",
    "                    \"如果找到公司，则输出公司名称, 输出格式例子：xxx公司\\n\\n\"\n",
    "                    \"{t}\\n\\n\"\n",
    "                )\n",
    "    idx = content.find(\"发行人：\")\n",
    "    max_token = 3000\n",
    "    if idx > 0 :\n",
    "        prompt = template.format_messages(t=content[idx:idx+max_token])\n",
    "    else:\n",
    "        idx = content.rfind(\"发行人简要情况\")\n",
    "        if idx > 0 :\n",
    "            prompt = template.format_messages(t=content[idx:idx+max_token])\n",
    "        else:\n",
    "            prompt = template.format_messages(t=content[:max_token])\n",
    "    # create a request activating streaming response\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt[0].content}\n",
    "        ],\n",
    "        temperature=0.6, top_p=1 \n",
    "    )\n",
    "    print(prompt[0].content)\n",
    "    ## 发行人： \n",
    "    print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查询文本理解题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import swifter\n",
    "\n",
    "import pandas as pd\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from model.ai_loader import client\n",
    "from utils import cosine_similarity\n",
    "from config import project_config\n",
    "from text_understanding import process_text_question\n",
    "\n",
    "questions = [\n",
    "    # \"确成硅化学股份有限公司的子公司无锡东沃经营范围是？\", # 空 \n",
    "    \"西安启源机电装备股份有限公司专用设备销售价格波动的主要原因是什么？\" # only one word\n",
    "]\n",
    "for question in questions:\n",
    "    questions_df = pd.read_csv(project_config.text_questions_path,index_col=0)\n",
    "    txtdf = pd.read_csv(project_config.company_file_path)\n",
    "    text_files_path = project_config.text_files_path\n",
    "    test_question_df = questions_df.loc[questions_df['question'] == question]\n",
    "    if len(test_question_df) > 0:\n",
    "        answer =  process_text_question(test_question_df.iloc[0], txtdf, text_files_path)\n",
    "        print(\"answer=\" + answer)\n",
    "    else:\n",
    "        print(\"no question found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查询数据查询题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.utilities import SQLDatabase\n",
    "# from model.ai_loader import client\n",
    "from config import project_config\n",
    "\n",
    "def create_sql_template():\n",
    "    return ChatPromptTemplate.from_template(\n",
    "        \"你是一个精通SQL语句的AI。\"\n",
    "        \"我会给你一个问题，请按照问题描述，只使用以下表格的表格名和列名写出正确的SQL代码，（数据只是样本，不代表数据库内所有数据，请按照具体问题的主体和要对应表名和字段名撰写SQL语句）。\\n\"\n",
    "        \"请直接生成能在以下sqlite3数据库中执行成功的SQL代码,不要有多余备注和假设。\\n\"\n",
    "        \"列名必须通过将名称括在后引号(`) 来分隔该名称, 必须指定对应的表名。\\n\"\n",
    "        \"请只使用以下表格的表格名和列名撰写SQL语句，如果多个问题可以生成多个sql语句来执行：\\n\\n\"\n",
    "        \"{db}\\n\\n\"\n",
    "        \"问题：{q}\\n\"\n",
    "        \"SQL语句：\\n\"\n",
    "    )\n",
    "def create_answer_template():\n",
    "    return ChatPromptTemplate.from_template(\n",
    "        \"你是一个会组织语言的AI。\"\n",
    "        \"我会给你一个问题，和相应的答案，请为我完整写出答案句\\n\"\n",
    "        \"问题：{q}\\n\"\n",
    "        \"答案：{a}\\n\"\n",
    "        \"答案句为：\"\n",
    "    )\n",
    "# 获取SQL回答\n",
    "def get_sql_answer(template, db, question):\n",
    "    prompt = template.format_messages(q=question, db=db.table_info)\n",
    "    resp = client.chat.completions.create(model=model_name, messages=[{\"role\": \"user\", \"content\": prompt[0].content}], temperature=project_config.temperature, top_p=project_config.top_p)\n",
    "    # print(resp.choices[0].message.content)\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "def process_sql_question(question):\n",
    "    if question is None or len(question):\n",
    "        return question\n",
    "    \n",
    "    # 进行字符串替换\n",
    "    # question = question.replace(\"昨日收盘价\", \"昨收盘\")\n",
    "    return question\n",
    "\n",
    "# 处理SQL答案\n",
    "def process_sql_answer(answer):\n",
    "    try:\n",
    "        ind = answer.index(\"SELECT\")\n",
    "        answer = answer[ind:]\n",
    "        ind = answer.index(';')\n",
    "        answer = answer[:ind + 1]\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    # 去除引号\n",
    "    if answer[0] in ['\"', \"'\"]:\n",
    "        answer = answer[1:]\n",
    "    if answer[-1] == '\"':\n",
    "        answer = answer[:-1]\n",
    "    if answer[-1] != ';':\n",
    "        answer += ';'\n",
    "\n",
    "    # 进行字符串替换\n",
    "    # answer = answer.replace('`',\"'\")\n",
    "    # answer = answer.replace(\"\\n\",\" \")\n",
    "\n",
    "    # answer = answer.replace(\"昨收盘(元)\", \"昨收盘\")\n",
    "    # answer = answer.replace(\"今开盘(元)\", \"今开盘\")\n",
    "    # answer = answer.replace(\"最高价(元)\", \"最高价\")\n",
    "    # answer = answer.replace(\"最低价(元)\", \"'最低价\")\n",
    "    # answer = answer.replace(\"收盘价(元)\", \"收盘价\")\n",
    "    # answer = answer.replace(\"成交量(股)\", \"成交量\")\n",
    "    # answer = answer.replace(\"成交金额(元)\", \"成交金额\")\n",
    "    # answer = answer.replace(\"所属国家(地区)\", \"所属国家\")\n",
    "\n",
    "    # answer = answer.replace(\"昨收盘\",\"昨收盘(元)\")\n",
    "    # answer = answer.replace(\"今开盘\", \"今开盘(元)\")\n",
    "    # answer = answer.replace(\"最高价\", \"最高价(元)\")\n",
    "    # answer = answer.replace(\"最低价\", \"'最低价(元)\")\n",
    "    # answer = answer.replace(\"收盘价\", \"收盘价(元)\")\n",
    "    # answer = answer.replace(\"成交量\", \"成交量(股)\")\n",
    "    # answer = answer.replace(\"成交金额\", \"成交金额(元)\")\n",
    "    # answer = answer.replace(\"所属国家\", \"所属国家(地区)\")\n",
    "\n",
    "    # answer = answer.replace(\"'昨收盘(元)'\",\"昨收盘(元)\")\n",
    "    # answer = answer.replace(\"'今开盘(元)'\", \"今开盘(元)\")\n",
    "    # answer = answer.replace(\"'最高价(元)'\", \"最高价(元)\")\n",
    "    # answer = answer.replace(\"'最低价(元)'\", \"最低价(元)\")\n",
    "    # answer = answer.replace(\"'收盘价(元)'\", \"收盘价(元)\")\n",
    "    # answer = answer.replace(\"'成交量(股)'\", \"成交量(股)\")\n",
    "    # answer = answer.replace(\"'成交金额(元)'\", \"成交金额(元)\")\n",
    "    # answer = answer.replace(\"'所属国家(地区)'\", \"所属国家(地区)\")\n",
    "\n",
    "    # answer = answer.replace(\"昨收盘(元)\",\"'昨收盘(元)'\")\n",
    "    # answer = answer.replace(\"今开盘(元)\", \"'今开盘(元)'\")\n",
    "    # answer = answer.replace(\"最高价(元)\", \"'最高价(元)'\")\n",
    "    # answer = answer.replace(\"最低价(元)\", \"'最低价(元)'\")\n",
    "    # answer = answer.replace(\"收盘价(元)\", \"'收盘价(元)'\")\n",
    "    # answer = answer.replace(\"成交量(股)\", \"'成交量(股)'\")\n",
    "    # answer = answer.replace(\"成交金额(元)\", \"'成交金额(元)'\")\n",
    "    # answer = answer.replace(\"所属国家(地区)\", \"'所属国家(地区)'\")\n",
    "\n",
    "    return answer\n",
    "\n",
    "# 获取格式化的答案\n",
    "def get_formatted_answer(template, question, answer):\n",
    "    prompt = template.format_messages(q=question, a=answer)\n",
    "    resp = client.chat.completions.create(model=model_name, messages=[{\"role\": \"user\", \"content\": prompt[0].content}], temperature=project_config.temperature, top_p=project_config.top_p)\n",
    "    return resp.choices[0].message.content\n",
    "# 执行SQL查询并获取结果\n",
    "def execute_sql_query(conn, query):\n",
    "    try:\n",
    "        result = conn.execute(query).fetchone()\n",
    "        return ', '.join(map(str, result)) if result else ''\n",
    "    except Exception as e:\n",
    "        return str(e)  # 或者返回空字符串\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///\"+ project_config.db_sqlite_url, sample_rows_in_table_info=2)\n",
    "sql_template = create_sql_template()\n",
    "# print(\"sql_template=\" + sql_template.messages[0].prompt.template)\n",
    "answer_template = create_answer_template()\n",
    "# print(\"\\nanswer_template=\" + answer_template[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "   \"请查询在2021年度，688338股票涨停天数？   解释：（收盘价/昨日收盘价-1）》=9.8% 视作涨停\",\n",
    "#    \"请帮我计算，在20210105，中信行业分类划分的一级行业为综合金融行业中，涨跌幅最大股票的股票代码是？涨跌幅是多少？百分数保留两位小数。股票涨跌幅定义为：（收盘价 - 前一日收盘价 / 前一日收盘价）* 100%。\"\n",
    "]\n",
    "start = 0\n",
    "questions_df = pd.read_csv(project_config.data_questions_path,index_col=0)[0:]\n",
    "for question in questions:\n",
    "    test_question_df = questions_df.loc[questions_df['question'] == question]\n",
    "    if len(test_question_df) > 0:\n",
    "        row = test_question_df.iloc[0]\n",
    "        answer =  process_sql_answer(get_sql_answer(sql_template, db, process_sql_question(row['question'])))\n",
    "        print(\"sql=\" + answer)\n",
    "        # 连接数据库\n",
    "        conn = sqlite3.connect(project_config.db_sqlite_url)\n",
    "        sql_result = execute_sql_query(conn, answer)\n",
    "        print(\"sql_result=\" + sql_result)\n",
    "        formatted_answer = get_formatted_answer(answer_template, row['question'], sql_result)\n",
    "        print(\"answer=\" + formatted_answer)\n",
    "    else:\n",
    "        print(\"no question found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin handle i=358, time=1701322251.6010182, question=我想知道红土创新稳进混合C基金在20201231的年报(含半年报)中，其可转债持仓占比最大的是哪个行业？用中信一级行业来统计。\n",
      "end handle i=358,end=1701322307.4513862, time=55.8503680229187, answer=根据红土创新稳进混合C基金在20201231年报(含半年报)显示，其可转债持仓占比最大的行业是公用事业，占比为20039386752.0。\n",
      "begin handle i=359, time=1701322312.60776, question=请查询：在2019的年度报告中，个人投资者持有基金份额大于机构投资者持有基金份额的基金属于股票型类型的有几个。\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-1106 in organization org-V9sN25iT8C07lapODoyyPbn3 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/jxy/Projects/person/ChallengeAI/2023bojin/demo.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jxy/Projects/person/ChallengeAI/2023bojin/demo.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jxy/Projects/person/ChallengeAI/2023bojin/demo.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mbegin handle i=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, time=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(start) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, question=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jxy/Projects/person/ChallengeAI/2023bojin/demo.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m answer \u001b[39m=\u001b[39m  process_sql_answer(get_sql_answer(sql_template, db, process_sql_question(row[\u001b[39m'\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m'\u001b[39;49m])))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jxy/Projects/person/ChallengeAI/2023bojin/demo.ipynb#X50sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m row[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m answer\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jxy/Projects/person/ChallengeAI/2023bojin/demo.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m sql_result \u001b[39m=\u001b[39m execute_sql_query(conn, row[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;32m/Users/jxy/Projects/person/ChallengeAI/2023bojin/demo.ipynb Cell 19\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jxy/Projects/person/ChallengeAI/2023bojin/demo.ipynb#X50sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_sql_answer\u001b[39m(template, db, question):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jxy/Projects/person/ChallengeAI/2023bojin/demo.ipynb#X50sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     prompt \u001b[39m=\u001b[39m template\u001b[39m.\u001b[39mformat_messages(q\u001b[39m=\u001b[39mquestion, db\u001b[39m=\u001b[39mdb\u001b[39m.\u001b[39mtable_info)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jxy/Projects/person/ChallengeAI/2023bojin/demo.ipynb#X50sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     resp \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(model\u001b[39m=\u001b[39;49mmodel_name, messages\u001b[39m=\u001b[39;49m[{\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mcontent}], temperature\u001b[39m=\u001b[39;49mproject_config\u001b[39m.\u001b[39;49mtemperature, top_p\u001b[39m=\u001b[39;49mproject_config\u001b[39m.\u001b[39;49mtop_p)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jxy/Projects/person/ChallengeAI/2023bojin/demo.ipynb#X50sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39m# print(resp.choices[0].message.content)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jxy/Projects/person/ChallengeAI/2023bojin/demo.ipynb#X50sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m resp\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/miniconda3/envs/stock/lib/python3.9/site-packages/openai/_utils/_utils.py:299\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 299\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/stock/lib/python3.9/site-packages/openai/resources/chat/completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    552\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    597\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    599\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    600\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    601\u001b[0m             {\n\u001b[1;32m    602\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    603\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    604\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    605\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    606\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    607\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    608\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    609\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    610\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    611\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    612\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    613\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    614\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    615\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    616\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    617\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    618\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    619\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    620\u001b[0m             },\n\u001b[1;32m    621\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    622\u001b[0m         ),\n\u001b[1;32m    623\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    624\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    625\u001b[0m         ),\n\u001b[1;32m    626\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    627\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    628\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    629\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/stock/lib/python3.9/site-packages/openai/_base_client.py:1063\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1050\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1051\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1059\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1060\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1061\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1062\u001b[0m     )\n\u001b[0;32m-> 1063\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/miniconda3/envs/stock/lib/python3.9/site-packages/openai/_base_client.py:842\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    834\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    835\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    840\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    841\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 842\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    843\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    844\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    845\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    846\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    847\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    848\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/stock/lib/python3.9/site-packages/openai/_base_client.py:873\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mHTTPStatusError \u001b[39mas\u001b[39;00m err:  \u001b[39m# thrown on 4xx and 5xx status code\u001b[39;00m\n\u001b[1;32m    872\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[0;32m--> 873\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m    874\u001b[0m             options,\n\u001b[1;32m    875\u001b[0m             cast_to,\n\u001b[1;32m    876\u001b[0m             retries,\n\u001b[1;32m    877\u001b[0m             err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    878\u001b[0m             stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    879\u001b[0m             stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    880\u001b[0m         )\n\u001b[1;32m    882\u001b[0m     \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    883\u001b[0m     \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniconda3/envs/stock/lib/python3.9/site-packages/openai/_base_client.py:933\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    931\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 933\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    934\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    935\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    936\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m    937\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    938\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    939\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/stock/lib/python3.9/site-packages/openai/_base_client.py:873\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mHTTPStatusError \u001b[39mas\u001b[39;00m err:  \u001b[39m# thrown on 4xx and 5xx status code\u001b[39;00m\n\u001b[1;32m    872\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[0;32m--> 873\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m    874\u001b[0m             options,\n\u001b[1;32m    875\u001b[0m             cast_to,\n\u001b[1;32m    876\u001b[0m             retries,\n\u001b[1;32m    877\u001b[0m             err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    878\u001b[0m             stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    879\u001b[0m             stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    880\u001b[0m         )\n\u001b[1;32m    882\u001b[0m     \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    883\u001b[0m     \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniconda3/envs/stock/lib/python3.9/site-packages/openai/_base_client.py:933\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    931\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 933\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    934\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    935\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    936\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m    937\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    938\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    939\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/stock/lib/python3.9/site-packages/openai/_base_client.py:885\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    883\u001b[0m     \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 885\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    887\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-1106 in organization org-V9sN25iT8C07lapODoyyPbn3 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = 358\n",
    "questions_df = pd.read_csv(project_config.data_questions_path,index_col=0)[start: start + 200]\n",
    "output = pd.DataFrame(columns=['id', 'question', 'formatted_answer'])\n",
    "# questions['answer'] = questions.apply(lambda row: process_sql_answer(get_sql_answer(sql_template, db, row['question'])), axis=1)\n",
    "# 连接数据库\n",
    "conn = sqlite3.connect(project_config.db_sqlite_url)\n",
    "for i, row in questions_df.iterrows():\n",
    "    start = time.time()\n",
    "    print(\"begin handle i=\" + str(i) + \", time=\" + str(start) + \", question=\" + row['question'])\n",
    "    answer =  process_sql_answer(get_sql_answer(sql_template, db, process_sql_question(row['question'])))\n",
    "    row['answer'] = answer\n",
    "    sql_result = execute_sql_query(conn, row['answer'])\n",
    "    formatted_answer = get_formatted_answer(answer_template, row['question'], sql_result)\n",
    "    output.at[i, 'id'] = row['id']\n",
    "    output.at[i, 'question'] = row['question']\n",
    "    output.at[i, 'formatted_answer'] = formatted_answer\n",
    "    end = time.time()\n",
    "    print(\"end handle i=\" + str(i) + \",end=\" + str(end) + \", time=\" + str(end - start) + \", answer=\" + formatted_answer)\n",
    "    if (end - start) < 61:\n",
    "        time.sleep(61 - (end - start))\n",
    "output.to_csv(project_config.data_answer_path + \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(project_config.data_answer_path + \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = 238\n",
    "questions_df = pd.read_csv(project_config.data_questions_path,index_col=0)[start: start + 200]\n",
    "output = pd.DataFrame(columns=['id', 'question', 'formatted_answer'])\n",
    "# questions['answer'] = questions.apply(lambda row: process_sql_answer(get_sql_answer(sql_template, db, row['question'])), axis=1)\n",
    "# 连接数据库\n",
    "conn = sqlite3.connect(project_config.db_sqlite_url)\n",
    "for i, row in questions_df.iterrows():\n",
    "    start = time.time()\n",
    "    print(\"begin handle i=\" + str(i) + \", time=\" + str(start) + \", question=\" + row['question'])\n",
    "    answer =  process_sql_answer(get_sql_answer(sql_template, db, process_sql_question(row['question'])))\n",
    "    row['answer'] = answer\n",
    "    sql_result = execute_sql_query(conn, row['answer'])\n",
    "    formatted_answer = get_formatted_answer(answer_template, row['question'], sql_result)\n",
    "    output.at[i, 'id'] = row['id']\n",
    "    output.at[i, 'question'] = row['question']\n",
    "    output.at[i, 'formatted_answer'] = formatted_answer\n",
    "    end = time.time()\n",
    "    print(\"end handle i=\" + str(i) + \",end=\" + str(end) + \", time=\" + str(end - start) + \", answer=\" + formatted_answer)\n",
    "    if (end - start) < 61:\n",
    "        time.sleep(61 - (end - start))\n",
    "output.to_csv(project_config.data_answer_path + \"3\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "sql = \"\"\"\n",
    "\"\"\".replace(\"\\\"\", \"\")\n",
    "print(sql)\n",
    "# 创建连接\n",
    "conn = sqlite3.connect(project_config.db_sqlite_url)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(sql)\n",
    "results = cursor.fetchall()\n",
    "for row in results:\n",
    "    print(row)\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sleep 不要关闭了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for x in range(60):\n",
    "  time.sleep(60)\n",
    "  print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
